{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d279ba6-c731-4ea7-8f33-127a34307ba8",
   "metadata": {},
   "source": [
    "# xarray analysis of 2017 data\n",
    "---\n",
    "\n",
    "I wrote the original analysis notebooks to use the pandas library, but also want to conduct this analysis in xarray to get more comfortable with the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6443c223-88c5-4c91-8a16-28d5b66203d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdb4dc-08bc-4fa8-9c9f-e2e721addc20",
   "metadata": {},
   "source": [
    "---\n",
    "### Pull in data\n",
    "Unfortunately, the `get_data` function from the OOI data lab turned the datasets into pandas dataframes before returning them, so I have to alter these functions to maintain xr dataset format. I can use the same links to grab the data, but it takes a few minutes to download it all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f92ab8-b7fd-4db7-af98-9ed285914225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, variables, deployments=None):\n",
    "    # Function to grab all data from specified directory\n",
    "    tds_url = 'https://opendap.oceanobservatories.org/thredds/dodsC'\n",
    "    dataset = requests.get(url).text\n",
    "    ii = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', dataset)\n",
    "    # x = re.findall(r'(ooi/.*?.nc)', dataset)\n",
    "    x = [y for y in ii if y.endswith('.nc')]\n",
    "    for i in x:\n",
    "        if i.endswith('.nc') == False:\n",
    "            x.remove(i)\n",
    "    for i in x:\n",
    "        try:\n",
    "            float(i[-4])\n",
    "        except:\n",
    "            x.remove(i)\n",
    "    # dataset = [os.path.join(tds_url, i) for i in x]\n",
    "    datasets = [os.path.join(tds_url, i.split('=')[-1]).replace(\"\\\\\",\"/\") for i in x]\n",
    "\n",
    "    # remove deployments not in deployment list, if given\n",
    "    if deployments is not None:\n",
    "        deploy = ['deployment{:04d}'.format(j) for j in deployments]\n",
    "        datasets = [k for k in datasets if k.split('/')[-1].split('_')[0] in deploy]\n",
    "\n",
    "    # remove collocated data files if necessary\n",
    "    catalog_rms = url.split('/')[-2][20:]\n",
    "    selected_datasets = []\n",
    "    for d in datasets:\n",
    "        if catalog_rms == d.split('/')[-1].split('_20')[0][15:]:\n",
    "            selected_datasets.append(d)\n",
    "\n",
    "    # create a dictionary to populate with data from the selected datasets\n",
    "    data_dict = {'time': np.array([], dtype='datetime64[ns]')}\n",
    "    unit_dict = {}\n",
    "    for v in variables:\n",
    "        data_dict.update({v: np.array([])})\n",
    "        unit_dict.update({v: []})\n",
    "    print('Appending data from files')\n",
    "\n",
    "    for sd in selected_datasets:\n",
    "        try:\n",
    "            url_with_fillmismatch = f'{sd}#fillmismatch'  # I had to add this line to get the function to work\n",
    "            ds = xr.open_dataset(url_with_fillmismatch, mask_and_scale=False)\n",
    "#             data_dict['time'] = np.append(data_dict['time'], ds['time'].values)\n",
    "            \n",
    "            for var in variables:\n",
    "#                 data_dict[var] = np.append(data_dict[var], ds[var].values)\n",
    "                units = ds[var].units\n",
    "                if units not in unit_dict[var]:\n",
    "                    unit_dict[var].append(units)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # convert dictionary to a dataframe\n",
    "#     df = pd.DataFrame(data_dict)\n",
    "#     df.sort_values(by=['time'], inplace=True)  # make sure the timestamps are in ascending order\n",
    "\n",
    "    ds = ds.swap_dims({'obs':'time'})\n",
    "    ds = ds.sortby(ds.time)\n",
    "\n",
    "    return ds, unit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1378c6-3282-40ae-a6a0-33ca8de2ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the variable(s) of interest\n",
    "METBK_2017_var = ['sea_surface_temperature', 'met_windavg_mag_corr_east', 'met_windavg_mag_corr_north']\n",
    "profiler_2017_var = ['seawater_pressure', 'density', 'practical_salinity', 'seawater_temperature', 'corrected_dissolved_oxygen']\n",
    "platform_2017_var = ['seawater_pressure', 'density', 'practical_salinity', 'seawater_temperature', 'dissolved_oxygen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c22592-04cb-4ab4-86c5-acbd50211925",
   "metadata": {},
   "outputs": [],
   "source": [
    "METBK_2017_url = 'https://opendap.oceanobservatories.org/thredds/catalog/ooi/deryag@uw.edu/20210422T030752259Z-CE04OSSM-SBD11-06-METBKA000-recovered_host-metbk_a_dcl_instrument_recovered/catalog.html'\n",
    "profiler_2017_url = 'https://opendap.oceanobservatories.org/thredds/catalog/ooi/deryag@uw.edu/20210422T030848056Z-CE04OSPS-SF01B-2A-CTDPFA107-streamed-ctdpf_sbe43_sample/catalog.html'\n",
    "platform_2017_url = 'https://opendap.oceanobservatories.org/thredds/catalog/ooi/deryag@uw.edu/20210428T021551666Z-CE04OSPS-PC01B-4A-CTDPFA109-streamed-ctdpf_optode_sample/catalog.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab4813-1125-4334-a648-ed6ea201dc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending data from files\n",
      "Appending data from files\n"
     ]
    }
   ],
   "source": [
    "# Get the data! \n",
    "METBK_2017_data, METBK_2017_units = get_data(METBK_2017_url, METBK_2017_var)\n",
    "profiler_2017_data, profiler_2017_units = get_data(profiler_2017_url, profiler_2017_var)\n",
    "platform_2017_data, platform_2017_units = get_data(platform_2017_url, platform_2017_var)\n",
    "\n",
    "# Check the variable units\n",
    "print(METBK_2017_units)\n",
    "print(profiler_2017_units)\n",
    "print(platform_2017_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad744c84-c293-4485-9d6b-f5b33be2111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "METBK_2017_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fee98-381a-49d8-854f-e15a2245c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "METBK_2017_data.to_netcdf('../../coastal_upwelling_output/metbk_data_2017.nc')\n",
    "profiler_2017_data.to_netcdf('../../coastal_upwelling_output/profiler_data_2017.nc')\n",
    "platform_2017_data.to_netcdf('../../coastal_upwelling_output/platform_data_2017.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
